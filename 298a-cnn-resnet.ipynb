{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6946700,"sourceType":"datasetVersion","datasetId":3989601},{"sourceId":7005884,"sourceType":"datasetVersion","datasetId":4027597}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LSTM**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime\n\n# Load your data here\ndata_path = '/kaggle/input/pig-disease-f/data 2/Train'\nmetadata = pd.read_csv('//kaggle/input/pig-data/data.csv')\n\nclass_names = {\n    1: 'Pneumonia',\n    2: 'PRRS',\n    3: 'Swine Fever'\n    # Add more class IDs and their corresponding names as needed\n}\n\nmetadata['class_name'] = metadata['classID'].map(class_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:20:59.475491Z","iopub.execute_input":"2023-12-17T01:20:59.475825Z","iopub.status.idle":"2023-12-17T01:21:17.020065Z","shell.execute_reply.started":"2023-12-17T01:20:59.475800Z","shell.execute_reply":"2023-12-17T01:21:17.019264Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming that your audio files are organized in folders named after their classes\nclass_folders = ['Pnemonia', 'prrs', 'swine_fever']\naudio_data = []\nlabels = []\n\nfor folder in class_folders:\n    folder_path = os.path.join(data_path, folder)\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.wav'):\n            audio_path = os.path.join(folder_path, filename)\n            audio, sr = librosa.load(audio_path, sr=None)\n            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n            audio_data.append(mfccs)\n            labels.append(folder)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:21:17.021529Z","iopub.execute_input":"2023-12-17T01:21:17.021808Z","iopub.status.idle":"2023-12-17T01:21:52.287915Z","shell.execute_reply.started":"2023-12-17T01:21:17.021784Z","shell.execute_reply":"2023-12-17T01:21:52.286458Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Padding MFCCs to ensure uniformity in feature dimensions\nmax_length = max([mfcc.shape[1] for mfcc in audio_data])\npadded_mfccs = [np.pad(mfcc, pad_width=((0, 0), (0, max_length - mfcc.shape[1])), mode='constant') for mfcc in audio_data]\n\n# Converting to NumPy arrays\nX = np.array(padded_mfccs)\ny = np.array(labels)\n\n# Reshape for LSTM input\nX = X.reshape(X.shape[0], X.shape[2], X.shape[1]) # Reshaping to (samples, timesteps, features)\n\n# Label encoding\nlabelencoder = LabelEncoder()\ny = to_categorical(labelencoder.fit_transform(y))\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:21:52.295343Z","iopub.execute_input":"2023-12-17T01:21:52.296316Z","iopub.status.idle":"2023-12-17T01:21:52.508639Z","shell.execute_reply.started":"2023-12-17T01:21:52.296267Z","shell.execute_reply":"2023-12-17T01:21:52.507840Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:21:52.510589Z","iopub.execute_input":"2023-12-17T01:21:52.510880Z","iopub.status.idle":"2023-12-17T01:21:58.702715Z","shell.execute_reply.started":"2023-12-17T01:21:52.510855Z","shell.execute_reply":"2023-12-17T01:21:58.701822Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 390, 128)          86528     \n                                                                 \n dropout (Dropout)           (None, 390, 128)          0         \n                                                                 \n lstm_1 (LSTM)               (None, 64)                49408     \n                                                                 \n dropout_1 (Dropout)         (None, 64)                0         \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 136131 (531.76 KB)\nTrainable params: 136131 (531.76 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 10\nnum_batch_size = 32\ncheckpointer = ModelCheckpoint(filepath='./audio_classification_lstm.hdf5', verbose=1, save_best_only=True)\n\nstart = datetime.now()\nmodel.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\nprint(\"Training completed in time: \", datetime.now() - start)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:21:58.703822Z","iopub.execute_input":"2023-12-17T01:21:58.704111Z","iopub.status.idle":"2023-12-17T01:22:22.627546Z","shell.execute_reply.started":"2023-12-17T01:21:58.704087Z","shell.execute_reply":"2023-12-17T01:22:22.626617Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/10\n24/26 [==========================>...] - ETA: 0s - loss: 0.7820 - accuracy: 0.7279\nEpoch 1: val_loss improved from inf to 0.55989, saving model to ./audio_classification_lstm.hdf5\n26/26 [==============================] - 12s 66ms/step - loss: 0.7700 - accuracy: 0.7318 - val_loss: 0.5599 - val_accuracy: 0.8107\nEpoch 2/10\n 7/26 [=======>......................] - ETA: 0s - loss: 0.6014 - accuracy: 0.7768","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"25/26 [===========================>..] - ETA: 0s - loss: 0.5561 - accuracy: 0.8062\nEpoch 2: val_loss improved from 0.55989 to 0.52743, saving model to ./audio_classification_lstm.hdf5\n26/26 [==============================] - 1s 28ms/step - loss: 0.5454 - accuracy: 0.8107 - val_loss: 0.5274 - val_accuracy: 0.8058\nEpoch 3/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.4630 - accuracy: 0.8338\nEpoch 3: val_loss improved from 0.52743 to 0.44859, saving model to ./audio_classification_lstm.hdf5\n26/26 [==============================] - 1s 28ms/step - loss: 0.4676 - accuracy: 0.8313 - val_loss: 0.4486 - val_accuracy: 0.8204\nEpoch 4/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.3421 - accuracy: 0.8863\nEpoch 4: val_loss improved from 0.44859 to 0.40664, saving model to ./audio_classification_lstm.hdf5\n26/26 [==============================] - 1s 28ms/step - loss: 0.3502 - accuracy: 0.8835 - val_loss: 0.4066 - val_accuracy: 0.8544\nEpoch 5/10\n26/26 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9235\nEpoch 5: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 32ms/step - loss: 0.2748 - accuracy: 0.9235 - val_loss: 0.4361 - val_accuracy: 0.8350\nEpoch 6/10\n24/26 [==========================>...] - ETA: 0s - loss: 0.2190 - accuracy: 0.9362\nEpoch 6: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 28ms/step - loss: 0.2240 - accuracy: 0.9333 - val_loss: 0.5241 - val_accuracy: 0.8010\nEpoch 7/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.1724 - accuracy: 0.9500\nEpoch 7: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 26ms/step - loss: 0.1725 - accuracy: 0.9502 - val_loss: 0.5689 - val_accuracy: 0.8398\nEpoch 8/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.1215 - accuracy: 0.9688\nEpoch 8: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 26ms/step - loss: 0.1185 - accuracy: 0.9697 - val_loss: 0.5840 - val_accuracy: 0.8155\nEpoch 9/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9675\nEpoch 9: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 26ms/step - loss: 0.1237 - accuracy: 0.9648 - val_loss: 0.6704 - val_accuracy: 0.8010\nEpoch 10/10\n25/26 [===========================>..] - ETA: 0s - loss: 0.0890 - accuracy: 0.9775\nEpoch 10: val_loss did not improve from 0.40664\n26/26 [==============================] - 1s 26ms/step - loss: 0.0870 - accuracy: 0.9782 - val_loss: 0.6731 - val_accuracy: 0.8155\nTraining completed in time:  0:00:23.917895\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f'Test accuracy: {test_accuracy[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:22.628842Z","iopub.execute_input":"2023-12-17T01:22:22.629173Z","iopub.status.idle":"2023-12-17T01:22:22.866357Z","shell.execute_reply.started":"2023-12-17T01:22:22.629145Z","shell.execute_reply":"2023-12-17T01:22:22.865435Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Test accuracy: 0.8155339956283569\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n# Predict classes\npredict_x = model.predict(X_test) \npredicted_classes = np.argmax(predict_x, axis=1)\n\n# Convert one-hot encoded y_test to label format\ntrue_classes = np.argmax(y_test, axis=1)\n# Confusion Matrix\nconf_matrix = confusion_matrix(true_classes, predicted_classes)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Precision\nprecision = precision_score(true_classes, predicted_classes, average='weighted')\nprint(f\"Precision: {precision}\")\n\n# Recall\nrecall = recall_score(true_classes, predicted_classes, average='weighted')\nprint(f\"Recall: {recall}\")\n\n# F1 Score\nf1 = f1_score(true_classes, predicted_classes, average='weighted')\nprint(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:22.867664Z","iopub.execute_input":"2023-12-17T01:22:22.868449Z","iopub.status.idle":"2023-12-17T01:22:23.708844Z","shell.execute_reply.started":"2023-12-17T01:22:22.868411Z","shell.execute_reply":"2023-12-17T01:22:23.707908Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"7/7 [==============================] - 1s 14ms/step\nConfusion Matrix:\n[[135  13   2]\n [ 16  12   2]\n [  5   0  21]]\nPrecision: 0.8060567587752054\nRecall: 0.8155339805825242\nF1 Score: 0.8099787134624371\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **ResNet**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.applications import ResNet50\nfrom datetime import datetime\n\n# Load your data here\ndata_path = '/kaggle/input/pig-disease-f/data 2/Train'\nmetadata = pd.read_csv('//kaggle/input/pig-data/data.csv')\n\nclass_names = {\n    1: 'Pneumonia',\n    2: 'PRRS',\n    3: 'Swine Fever'\n}\n\nmetadata['class_name'] = metadata['classID'].map(class_names)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-17T01:22:23.710125Z","iopub.execute_input":"2023-12-17T01:22:23.710484Z","iopub.status.idle":"2023-12-17T01:22:23.725428Z","shell.execute_reply.started":"2023-12-17T01:22:23.710451Z","shell.execute_reply":"2023-12-17T01:22:23.724572Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Assuming that your audio files are organized in folders named after their classes\nclass_folders = ['Pnemonia', 'prrs', 'swine_fever']\naudio_data = []\nlabels = []\n\nfor folder in class_folders:\n    folder_path = os.path.join(data_path, folder)\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.wav'):\n            audio_path = os.path.join(folder_path, filename)\n            audio, sr = librosa.load(audio_path, sr=None)\n            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n            audio_data.append(mfccs)\n            labels.append(folder)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:23.726474Z","iopub.execute_input":"2023-12-17T01:22:23.726786Z","iopub.status.idle":"2023-12-17T01:22:39.347324Z","shell.execute_reply.started":"2023-12-17T01:22:23.726761Z","shell.execute_reply":"2023-12-17T01:22:39.346011Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class_folders = ['Pnemonia', 'prrs', 'swine_fever']\naudio_data = []\nlabels = []\n\nfor folder in class_folders:\n    folder_path = os.path.join(data_path, folder)\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.wav'):\n            audio_path = os.path.join(folder_path, filename)\n            audio, sr = librosa.load(audio_path, sr=None)\n            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n            audio_data.append(mfccs)\n            labels.append(folder)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:39.351142Z","iopub.execute_input":"2023-12-17T01:22:39.351966Z","iopub.status.idle":"2023-12-17T01:22:55.545098Z","shell.execute_reply.started":"2023-12-17T01:22:39.351932Z","shell.execute_reply":"2023-12-17T01:22:55.543758Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Padding MFCCs to ensure uniformity in feature dimensions\nmax_length = max([mfcc.shape[1] for mfcc in audio_data])\npadded_mfccs = [np.pad(mfcc, pad_width=((0, 0), (0, max_length - mfcc.shape[1])), mode='constant') for mfcc in audio_data]\n\n# Converting to NumPy arrays\nX = np.array(padded_mfccs)\ny = np.array(labels)\n\n# Reshape for ResNet input\n# Assuming that the size of the MFCCs is (40, max_length), we need to add a channel dimension\nX_reshaped = np.expand_dims(X, axis=-1)\n\n# Label encoding\nlabelencoder = LabelEncoder()\ny = to_categorical(labelencoder.fit_transform(y))\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:55.552625Z","iopub.execute_input":"2023-12-17T01:22:55.556528Z","iopub.status.idle":"2023-12-17T01:22:55.753897Z","shell.execute_reply.started":"2023-12-17T01:22:55.556463Z","shell.execute_reply":"2023-12-17T01:22:55.752949Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]  # Shape of input data\n\nbase_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:55.755171Z","iopub.execute_input":"2023-12-17T01:22:55.755491Z","iopub.status.idle":"2023-12-17T01:22:57.595444Z","shell.execute_reply.started":"2023-12-17T01:22:55.755463Z","shell.execute_reply":"2023-12-17T01:22:57.594639Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n resnet50 (Functional)       (None, 2, 13, 2048)       23581440  \n                                                                 \n global_average_pooling2d (  (None, 2048)              0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense_1 (Dense)             (None, 256)               524544    \n                                                                 \n batch_normalization (Batch  (None, 256)               1024      \n Normalization)                                                  \n                                                                 \n dense_2 (Dense)             (None, 3)                 771       \n                                                                 \n=================================================================\nTotal params: 24107779 (91.96 MB)\nTrainable params: 24054147 (91.76 MB)\nNon-trainable params: 53632 (209.50 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 10\nnum_batch_size = 32\ncheckpointer = ModelCheckpoint(filepath='./audio_classification_resnet.hdf5', verbose=1, save_best_only=True)\n\nstart = datetime.now()\nmodel.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\nprint(\"Training completed in time: \", datetime.now() - start)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:22:57.599729Z","iopub.execute_input":"2023-12-17T01:22:57.600183Z","iopub.status.idle":"2023-12-17T01:24:27.952884Z","shell.execute_reply.started":"2023-12-17T01:22:57.600138Z","shell.execute_reply":"2023-12-17T01:24:27.951991Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/10\n26/26 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.5522\nEpoch 1: val_loss improved from inf to 64.96595, saving model to ./audio_classification_resnet.hdf5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"26/26 [==============================] - 52s 331ms/step - loss: 1.0789 - accuracy: 0.5522 - val_loss: 64.9660 - val_accuracy: 0.7282\nEpoch 2/10\n26/26 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.8034\nEpoch 2: val_loss improved from 64.96595 to 2.96689, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 176ms/step - loss: 0.5808 - accuracy: 0.8034 - val_loss: 2.9669 - val_accuracy: 0.1942\nEpoch 3/10\n26/26 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.8507\nEpoch 3: val_loss improved from 2.96689 to 1.20774, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 177ms/step - loss: 0.4009 - accuracy: 0.8507 - val_loss: 1.2077 - val_accuracy: 0.4757\nEpoch 4/10\n26/26 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9090\nEpoch 4: val_loss improved from 1.20774 to 0.96371, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 177ms/step - loss: 0.2578 - accuracy: 0.9090 - val_loss: 0.9637 - val_accuracy: 0.7233\nEpoch 5/10\n26/26 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9308\nEpoch 5: val_loss improved from 0.96371 to 0.77476, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 176ms/step - loss: 0.2128 - accuracy: 0.9308 - val_loss: 0.7748 - val_accuracy: 0.7379\nEpoch 6/10\n26/26 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9393\nEpoch 6: val_loss did not improve from 0.77476\n26/26 [==============================] - 3s 132ms/step - loss: 0.1958 - accuracy: 0.9393 - val_loss: 0.8274 - val_accuracy: 0.7282\nEpoch 7/10\n26/26 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9527\nEpoch 7: val_loss did not improve from 0.77476\n26/26 [==============================] - 3s 133ms/step - loss: 0.1307 - accuracy: 0.9527 - val_loss: 1.3622 - val_accuracy: 0.7282\nEpoch 8/10\n26/26 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9587\nEpoch 8: val_loss improved from 0.77476 to 0.74527, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 180ms/step - loss: 0.1369 - accuracy: 0.9587 - val_loss: 0.7453 - val_accuracy: 0.8010\nEpoch 9/10\n26/26 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9636\nEpoch 9: val_loss did not improve from 0.74527\n26/26 [==============================] - 3s 134ms/step - loss: 0.1285 - accuracy: 0.9636 - val_loss: 0.8202 - val_accuracy: 0.8155\nEpoch 10/10\n26/26 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9551\nEpoch 10: val_loss improved from 0.74527 to 0.55798, saving model to ./audio_classification_resnet.hdf5\n26/26 [==============================] - 5s 179ms/step - loss: 0.1283 - accuracy: 0.9551 - val_loss: 0.5580 - val_accuracy: 0.8058\nTraining completed in time:  0:01:30.346757\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f'Test accuracy: {test_accuracy[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:24:27.954100Z","iopub.execute_input":"2023-12-17T01:24:27.954396Z","iopub.status.idle":"2023-12-17T01:24:28.256230Z","shell.execute_reply.started":"2023-12-17T01:24:27.954370Z","shell.execute_reply":"2023-12-17T01:24:28.255326Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Test accuracy: 0.8058252334594727\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n# Predict classes\npredict_x = model.predict(X_test) \npredicted_classes = np.argmax(predict_x, axis=1)\n\n# Convert one-hot encoded y_test to label format\ntrue_classes = np.argmax(y_test, axis=1)\n# Confusion Matrix\nconf_matrix = confusion_matrix(true_classes, predicted_classes)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Precision\nprecision = precision_score(true_classes, predicted_classes, average='weighted')\nprint(f\"Precision: {precision}\")\n\n# Recall\nrecall = recall_score(true_classes, predicted_classes, average='weighted')\nprint(f\"Recall: {recall}\")\n\n# F1 Score\nf1 = f1_score(true_classes, predicted_classes, average='weighted')\nprint(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:24:28.257447Z","iopub.execute_input":"2023-12-17T01:24:28.257811Z","iopub.status.idle":"2023-12-17T01:24:29.484006Z","shell.execute_reply.started":"2023-12-17T01:24:28.257777Z","shell.execute_reply":"2023-12-17T01:24:29.482512Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"7/7 [==============================] - 1s 34ms/step\nConfusion Matrix:\n[[139   0  11]\n [ 26   1   3]\n [  0   0  26]]\nPrecision: 0.8410856134157105\nRecall: 0.8058252427184466\nF1 Score: 0.7514625472658671\n","output_type":"stream"}]}]}